# -*- coding: utf-8 -*-
"""Machine learning analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11WoqNFR5Pa98FZ2qIQE3J3HBIUqvlf66
"""

from google.colab import drive
drive.mount('/content/drive')



import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/churn.csv")

df.info()

df.head()

df.isnull().head()

df.isnull().sum()

df.dtypes

import matplotlib.pyplot as plt

print(df.Exited.value_counts())

p=df.Exited.value_counts().plot(kind="bar")

df['Geography'] = df['Geography'].replace(['Germany'],'0')
df['Geography'] = df['Geography'].replace(['France'],'1')
df['Geography'] = df['Geography'].replace(['Spain'],'2')
#Change value in gender column
df['Gender'] = df['Gender'].replace(['Female'],'0')
df['Gender'] = df['Gender'].replace(['Male'],'1')
df.head()

p=df.Exited.value_counts().plot(kind="bar")

df['Geography'] = df['Geography'].replace(['Germany'],'0')
df['Geography'] = df['Geography'].replace(['France'],'1')
df['Geography'] = df['Geography'].replace(['Spain'],'2')

df['Geography'] = pd.to_numeric(df['Geography'])
df['Gender'] = pd.to_numeric(df['Gender'])
df.dtypes

new_df = df.drop(['Geography','RowNumber','Surname','CustomerId'], axis = 1)

new_df

new_df.head()

import seaborn as sns
import matplotlib.pyplot as plt

X = new_df.drop('Exited', axis=1)
y = new_df['Exited']
#train size 80% and test size 20%
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from sklearn.metrics import accuracy_score
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,random_state=42)

from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()
dtree.fit(X_train, y_train)
y_pred = dtree.predict(X_test)
print("Accuracy Score :", accuracy_score(y_test, y_pred)*100, "%")

#Random Forest
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)
y_pred = rfc.predict(X_test)
print("Accuracy Score :", accuracy_score(y_test, y_pred)*100, "%")

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

rf_modelWithoob = RandomForestClassifier(random_state=42, oob_score=True)
rf_modelWithoob.fit(X_train, y_train)
oob_probs = rf_modelWithoob.oob_decision_function_
oob_preds = [probs.argmax() for probs in oob_probs]
oob_mse = mean_squared_error(y_train, oob_preds)
print('OOB MSE score:', oob_mse)
print('OOB R2 score:', rf_modelWithoob.oob_score_)

importances = pd.Series(rf_modelWithoob.feature_importances_, index=X.columns)
print(importances)
importances_sorted = importances.sort_values()
importances_sorted.plot(kind='barh')
plt.show

# create an instance of SMOTETomek
smote_tomek = SMOTETomek(random_state=42)

# fit and transform the training data using SMOTETomek
X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)


# create an instance of SVM classifier
svm = SVC(kernel='linear', C=1, random_state=42)

# fit the SVM classifier on the resampled training data
svm.fit(X_train_resampled, y_train_resampled)

# make predictions on the testing data
y_pred = svm.predict(X_test)

# evaluate the performance of the classifier using classification report and confusion matrix
print('Classification Report:\n', classification_report(y_test, y_pred))
print('Confusion Matrix:\n', confusion_matrix(y_test, y_pred))

#Support Vector Machine
from sklearn import svm
svm = svm.SVC()
svm.fit(X_train, y_train)
y_pred = svm.predict(X_test)
print("Accuracy Score :", accuracy_score(y_test, y_pred)*100, "%")

# SVM using SmoteTomek

from imblearn.under_sampling import TomekLinks
tl = TomekLinks()
X_train_tl, y_train_tl = tl.fit_resample(X_train, y_train)

# Initialize your svm classifier
svm = svm.SVC(kernel='linear',C = 1)

# Train the svm classifier on the training set with Tomek Links applied
svm.fit(X_train_tl, y_train_tl)

# Make predictions on the testing set
y_pred = svm.predict(X_test)

# Evaluate the performance of the classifier
from sklearn.metrics import accuracy_score, classification_report
print("Accuracy Score :", accuracy_score(y_test, y_pred)*100, "%")
print(classification_report(y_test, y_pred))

#XGBoost
from xgboost import XGBClassifier
xgb_model = XGBClassifier()
xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_test)
print("Accuracy Score :", accuracy_score(y_test, y_pred)*100, "%")

# xgboost using SmoteTomek

from imblearn.under_sampling import TomekLinks
tl = TomekLinks()
X_train_tl, y_train_tl = tl.fit_resample(X_train, y_train)

# Initialize your Random Forest classifier
rf = RandomForestClassifier()

# Train the Random Forest classifier on the training set with Tomek Links applied
rf.fit(X_train_tl, y_train_tl)

# Make predictions on the testing set
y_pred = rf.predict(X_test)

# Evaluate the performance of the classifier
from sklearn.metrics import accuracy_score, classification_report
print("Accuracy Score :", accuracy_score(y_test, y_pred)*100, "%")
print(classification_report(y_test, y_pred))

#Visualize Random Forest and XGBoost Algorithm because Random Forest and XGBoost Algorithm have the Best Accuracy
#importing classification report and confusion matrix from sklearn

from sklearn.metrics import classification_report, confusion_matrix

"""Random Forest"""

y_pred = rfc.predict(X_test)
print("Classification report - n")

classification_report(y_test,y_pred)
print(classification_report(y_test,y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,5))
sns.heatmap(data=cm,linewidths=.5, annot=True,square = True,  cmap = 'Blues')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
all_sample_title = 'Accuracy Score: {0}'.format(rfc.score(X_test, y_test))
plt.title(all_sample_title, size = 15)

from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV,\
RandomizedSearchCV

# Define the parameter grid to search over
param_grid = {'n_estimators': [50, 100, 200, 300, 400],
              'max_depth': [5, 10, 15, 20]}

grid_search = GridSearchCV(RandomForestClassifier(),
                           param_grid=param_grid)
grid_search.fit(X_train, y_train)
print(grid_search.best_estimator_)

# random forest for feature importance on a classification problem
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from matplotlib import pyplot

rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

importances = pd.Series(rf_model.feature_importances_, index=X.columns)
print(importances)
importances.plot(kind='barh')
plt.show()

import numpy as np
from sklearn.metrics import roc_curve, roc_auc_score
y_pred_proba = rfc.predict_proba(X_test)[:][:,1]
df_actual_predicted = pd.concat([pd.DataFrame(np.array(y_test), columns=['y_actual']), pd.DataFrame(y_pred_proba, columns=['y_pred_proba'])], axis=1)
df_actual_predicted.index = y_test.index
fpr, tpr, tr = roc_curve(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])
auc = roc_auc_score(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])
plt.plot(fpr, tpr, label='AUC = %0.4f' %auc)
plt.plot(fpr, fpr, linestyle = '--', color='k')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve', size = 15)
plt.legend()

y_pred = xgb_model.predict(X_test)
print("Classification report - n")

print(classification_report(y_test,y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,5))
sns.heatmap(data=cm,linewidths=.5, annot=True,square = True,  cmap = 'Blues')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
all_sample_title = 'Accuracy Score: {0}'.format(xgb_model.score(X_test, y_test))
plt.title(all_sample_title, size = 15)

# print the classification results
print("Classification Results:")
results_df = pd.DataFrame(results, columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Best Parameters'])
display(results_df)

import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import xgboost as xgb


# create a pipeline for all classification models with hyperparameter tuning using Grid Search CV
classifiers = [
    ('SVC',SVC(random_state=42)),

    ('Decision Tree', DecisionTreeClassifier(random_state=42)),
    ('Random Forest', RandomForestClassifier(random_state=42))
]


param_grids = [
    {'classifier__C': [0.001, 0.01, 0.1, 1, 10]},
    {'classifier__max_depth': [3, 5, 7, 10, None]},
    {'classifier__n_estimators': [50, 100, 200, 300, 500]}
]


results = []
best_models = []
for i, (classifier_name, classifier) in enumerate(classifiers):
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', classifier)
    ])
    param_grid = param_grids[i]
    grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='f1')
    grid_search.fit(X_train, y_train)
    y_pred = grid_search.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    results.append([classifier_name, accuracy, precision, recall, f1, grid_search.best_params_])
    best_models.append(grid_search.best_estimator_)


# print the classification results
print("Classification Results:")
results_df = pd.DataFrame(results, columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Best Parameters'])
display(results_df)

import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score



# create a pipeline for all classification models with hyperparameter tuning using Grid Search CV
classifiers = [
    ('SVC',SVC(random_state=42)),
    ('Decision Tree', DecisionTreeClassifier(random_state=42)),
    ('Random Forest', RandomForestClassifier(random_state=42))
]


param_grids = [
    {'classifier__C': [0.001, 0.01, 0.1, 1, 10]},
    {'classifier__max_depth': [3, 5, 7, 10, None]},
    {'classifier__n_estimators': [50, 100, 200, 300, 500]}
]


results = []
best_models = []
for i, (classifier_name, classifier) in enumerate(classifiers):
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', classifier)
    ])
    param_grid = param_grids[i]
    grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='f1')
    grid_search.fit(X_train, y_train)
    y_pred = grid_search.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    results.append([classifier_name, accuracy, precision, recall, f1, grid_search.best_params_])
    best_models.append(grid_search.best_estimator_)


# print the classification results
print("Classification Results:")
results_df = pd.DataFrame(results, columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Best Parameters'])
display(results_df)

best_model = best_models[results_df['F1 Score'].idxmax()]
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)


print("\nBest Model Estimator:")
print(best_model)
print("\nBest Model Evaluation Results:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

