# -*- coding: utf-8 -*-
"""descriptive statistics project6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17Pdzpdhr8KJ39eabXtQvMmSIBJR3uSo4
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("/content/drive/MyDrive/churn.csv")

df.info()

df.head()

df.describe

df.describe()

remove_features = ["CustomerId", "Surname", "RowNumber"]
 df.drop(remove_features, axis = 1, inplace = True)

df.info()

print(len(df),"\n")
print(df.shape,"\n")
print(df.index,"\n")
print(df.columns,"\n")
print(df.info(),"\n")
print(df.count(),"\n")

df.head(5)

df.describe

df.describe()

df.describe().transpose()

df.mean()

df.describe().IsActiveMember

df.describe().HasCrCard

df.describe().Age

df.describe().CreditScore

df.describe().Balance

df.describe().EstimatedSalary

df.describe().Tenure

df.skew()

df.kurtosis()

df.Age.plot.hist()

df.CreditScore.plot.hist()

df.EstimatedSalary.plot.hist()

df.Tenure.plot.hist()

df.Balance.plot.hist()

df.NumOfProducts.plot.hist()

import matplotlib.pyplot as plt

df.Exited.plot.hist()

df.IsActiveMember.plot.hist()

df.IsActiveMember.plot.kde()

df.EstimatedSalary.plot.kde()

~why is the line variable in this manner?
df.EstimatedSalary.plot()

df.EstimatedSalary.plot.box()

#note the box plot and understand the scaling and standardaization of box plot

df.Age.plot.box()

df.plot.area()

df.plot.scatter(x='Age', y ='CreditScore')

from scipy.stats import pearsonr
import seaborn as sns

"""to go over the correlation for scatterplot"""

sns.scatterplot(x ='Age', y ='Exited', data = df )

# (df = pd.get_dummies(df, columns=['Gender', 'Exited']))
df['Gender'] = (df['Gender']== 'Male').astype(int)

# def make_plot(truth, prediction):
#   plt.plot(truth, color="red", label="truth")
#   plt.plot(prediction, color="blue", label="predicted")
#   plt.legend()
#   plt.grid()
#   plt.title("Comparing truth and predicted regression values")
#   plt.tight_layout()
#   plt.show()

labels = 'Exited', 'Not Exited'
sizes = [df.Exited[df['Exited']==1].count(), df.Exited[df['Exited']==0].count()]
explode = (0, 0.1)
fig1, ax1 = plt.subplots(figsize=(10, 8))
ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)
ax1.axis('equal')
plt.title("Proportion of customer churned and retained", size = 20)
plt.show()

fig, axarr = plt.subplots(2, 2, figsize=(20, 12))
sns.countplot(x='EstimatedSalary', hue = 'Exited',data = df, ax=axarr[0][0])
sns.countplot(x='Gender', hue = 'Exited',data = df, ax=axarr[0][1])
sns.countplot(x='HasCrCard', hue = 'Exited',data = df, ax=axarr[1][0])
sns.countplot(x='IsActiveMember', hue = 'Exited',data = df, ax=axarr[1][1])
sns.countplot(x='CreditScore', hue = 'Exited',data = df, ax=axarr[0][0])

# Relations based on the continuous data attributes
fig, axarr = plt.subplots(3, 2, figsize=(20, 12))
sns.boxplot(y='CreditScore',x = 'Exited', hue = 'Exited',data = df, ax=axarr[0][0])
sns.boxplot(y='Age',x = 'Exited', hue = 'Exited',data = df , ax=axarr[0][1])
sns.boxplot(y='Tenure',x = 'Exited', hue = 'Exited',data = df, ax=axarr[1][0])
sns.boxplot(y='Balance',x = 'Exited', hue = 'Exited',data = df, ax=axarr[1][1])
sns.boxplot(y='NumOfProducts',x = 'Exited', hue = 'Exited',data = df, ax=axarr[2][0])
sns.boxplot(y='EstimatedSalary',x = 'Exited', hue = 'Exited',data = df, ax=axarr[2][1])

# Split Train, test data
df_train = df.sample(frac=0.8,random_state=200)
df_test = df.drop(df_train.index)
print(len(df_train))
print(len(df_test))

df_train['BalanceSalaryRatio'] = df_train.Balance/df_train.EstimatedSalary
sns.boxplot(y='BalanceSalaryRatio',x = 'Exited', hue = 'Exited',data = df_train)
plt.ylim(-1, 5)

# Given that tenure is a 'function' of age, we introduce a variable aiming to standardize tenure over age:
df_train['TenureByAge'] = df_train.Tenure/(df_train.Age)
sns.boxplot(y='TenureByAge',x = 'Exited', hue = 'Exited',data = df_train)
plt.ylim(-1, 1)
plt.show()

df_train['CreditScoreGivenAge'] = df_train.CreditScore/(df_train.Age)

# Resulting Data Frame
df_train.head()

# Arrange columns by data type for easier manipulation
continuous_vars = ['CreditScore',  'Age', 'Tenure', 'Balance','NumOfProducts', 'EstimatedSalary', 'BalanceSalaryRatio',
                   'TenureByAge','CreditScoreGivenAge']
cat_vars = ['HasCrCard', 'IsActiveMember','Geography', 'Gender']
df_train = df_train[['Exited'] + continuous_vars + cat_vars]
df_train.head()

df_train.loc[df_train.HasCrCard == 0, 'HasCrCard'] = -1
df_train.loc[df_train.IsActiveMember == 0, 'IsActiveMember'] = -1
df_train.head()

# One hot encode the categorical variables
lst = ['Geography', 'Gender']
remove = list()
for i in lst:
    if (df_train[i].dtype == np.str or df_train[i].dtype == np.object):
        for j in df_train[i].unique():
            df_train[i+'_'+j] = np.where(df_train[i] == j,1,-1)
        remove.append(i)
df_train = df_train.drop(remove, axis=1)
df_train.head()

# minMax scaling the continuous variables
minVec = df_train[continuous_vars].min().copy()
maxVec = df_train[continuous_vars].max().copy()
df_train[continuous_vars] = (df_train[continuous_vars]-minVec)/(maxVec-minVec)
df_train.head()

from sklearn.datasets import make_classification

X, y = make_classification(n_samples=10000, n_features= 11, n_informative=10,
                           n_redundant=0, n_classes=2, weights=[0.9, 0.1], random_state=42)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

from imblearn.under_sampling import TomekLinks

tl = TomekLinks()
X_train_tl, y_train_tl = tl.fit_resample(X_train, y_train)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_tl, y_train_tl)
y_pred = rf.predict(X_test)

print(classification_report(y_test, y_pred))

from imblearn.combine import SMOTETomek
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize SMOTETomek
smt = SMOTETomek(random_state=42)

# Resample the training data using SMOTETomek
X_train_res, y_train_res = smt.fit_resample(X_train, y_train)

# Initialize the SVM classifier
clf = SVC()

# Train the SVM classifier on the resampled data
clf.fit(X_train_res, y_train_res)

# Make predictions on the testing set
y_pred = clf.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))
